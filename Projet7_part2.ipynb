{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "Projet7_part2.ipynb",
      "authorship_tag": "ABX9TyNgb7aTkXDs5JKuYLT2JTKQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Loryana/OCProjet7/blob/main/Projet7_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKFiAAdYhuWE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import time\n",
        "from contextlib import contextmanager\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zmS2z4PliKBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = df_scaled[df_scaled['TARGET'].notnull()]\n",
        "test_df = df_scaled[df_scaled['TARGET'].isnull()]\n",
        "print(\"Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
        "\n",
        "feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
        "\n",
        "train_x, train_y = train_df[feats], train_df['TARGET']\n",
        "valid_x, valid_y = train_df[feats], train_df['TARGET']\n",
        "#clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],eval_metric= 'auc')\n"
      ],
      "metadata": {
        "id": "Q3RSj3lCiJcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "# Score métier\n",
        "def custom_metric(y, y_pred):\n",
        "\n",
        "    # Matrice de Confusion\n",
        "    mat_conf = confusion_matrix(y,y_pred)\n",
        "\n",
        "    TP = mat_conf[0, 0]\n",
        "    FP = mat_conf[0, 1]\n",
        "    FN = mat_conf[1, 0]\n",
        "    TN = mat_conf[1, 1]\n",
        "\n",
        "    score = (10*FN + FP)\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "0BGyJaebiLyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import fbeta_score, make_scorer, precision_score,recall_score\n",
        "from sklearn.metrics import make_scorer\n",
        "# création des scores avec make_scorer()\n",
        "custom_scorer = make_scorer(custom_metric, greater_is_better=False)\n",
        "\n",
        "f1_scorer = make_scorer(fbeta_score, beta=1, greater_is_better=True)\n",
        "f2_scorer = make_scorer(fbeta_score, beta=2, greater_is_better=True)\n",
        "\n",
        "precision_scorer = make_scorer(precision_score, greater_is_better=True)\n",
        "recall_scorer = make_scorer(recall_score, greater_is_better=True)\n",
        "roc_auc_scorer = make_scorer(roc_auc_score, greater_is_better=True, needs_proba=True)"
      ],
      "metadata": {
        "id": "TB2KI-yDiMrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "def model_eval_score(model, X_test, y_test, df_scores=None, grid = True):\n",
        "\n",
        "    y_pred = model.predict(X_test) #returns a single discrete category that it has predicted\n",
        "    y_pred_proba = model.predict_proba(X_test)[:,1] #returns continuous values that represent the likelihood of each input belonging to each class\n",
        "\n",
        "    #on calcule la matrice de confusion et on l'affiche\n",
        "    conf_mat = confusion_matrix(y_test, y_pred)\n",
        "    print(conf_mat)\n",
        "\n",
        "    if grid == False:\n",
        "        dict_score_func = {'Modèle' : type(model).__name__,\n",
        "                       'Accuracy score': (accuracy_score(y_test, y_pred)),\n",
        "                       'Recall score': (recall_score(y_test, y_pred)),\n",
        "                       'Precision score': (precision_score(y_test, y_pred)),\n",
        "                       'F1 score': (f1_score(y_test, y_pred)),\n",
        "                       'F2 score': (fbeta_score(y_test, y_pred, beta= 2)),\n",
        "                       'Score metier':  (custom_metric(y_test, y_pred)),\n",
        "                       'ROCAUC score': (roc_auc_score(y_test, y_pred_proba)),\n",
        "                       'FN': conf_mat[1, 0],\n",
        "                       'FP': conf_mat[0, 1],\n",
        "                      }\n",
        "\n",
        "        list_columns = ['Modèle','Accuracy score','Recall score','Precision score',\n",
        "                    'F1 score','F2 score','Score metier','ROCAUC score',\n",
        "                   'FN', 'FP']\n",
        "\n",
        "        df_scores_rs = pd.DataFrame()\n",
        "\n",
        "    if grid == True:\n",
        "\n",
        "        df_scores_rs = pd.DataFrame(model.cv_results_).sort_values(by ='rank_test_score', ascending = True).reset_index(drop=True)\n",
        "        df_scores_rs = df_scores_rs[['rank_test_score','mean_fit_time','param_classifier','params', 'split0_test_score',\n",
        "                             'split1_test_score', 'split2_test_score',\n",
        "                             'mean_test_score', 'std_test_score']]\n",
        "\n",
        "        dict_score_func = {'Modèle' : model.best_estimator_,\n",
        "                           'Accuracy score': (accuracy_score(y_test, y_pred)),\n",
        "                           'Recall score': (recall_score(y_test, y_pred)),\n",
        "                           'Precision score': (precision_score(y_test, y_pred)),\n",
        "                            'F1 score': (f1_score(y_test, y_pred)),\n",
        "                           'F2 score': (fbeta_score(y_test, y_pred, beta= 2)),\n",
        "                           'Score metier':  (custom_metric(y_test, y_pred)),\n",
        "                           'ROCAUC score': (roc_auc_score(y_test, y_pred_proba)),\n",
        "                           'FN': conf_mat[1, 0],\n",
        "                           'FP': conf_mat[0, 1],\n",
        "                           'fit time': df_scores_rs.loc[0,'mean_fit_time']\n",
        "                      }\n",
        "\n",
        "        list_columns = ['Modèle','Accuracy score','Recall score','Precision score',\n",
        "                    'F1 score','F2 score','Score metier','ROCAUC score',\n",
        "                   'FN', 'FP','fit time']\n",
        "\n",
        "\n",
        "    if df_scores is None:\n",
        "        df_scores = pd.DataFrame(columns = list_columns)\n",
        "\n",
        "    df_scores = pd.concat([df_scores, pd.DataFrame(dict_score_func, index = [0])]).reset_index(drop=True)\n",
        "\n",
        "    return df_scores , df_scores_rs"
      ],
      "metadata": {
        "id": "1ndBE3W9iRY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "mod1 = RandomForestClassifier(random_state=27)\n",
        "mod1.fit(train_x, train_y)\n",
        "df_scores, df0 = model_eval_score(mod1, valid_x, valid_y, grid = False)\n",
        "\n",
        "df_scores"
      ],
      "metadata": {
        "id": "LgVIEcVjiTO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_eval(model, X_test, y_test):\n",
        "\n",
        "    y_pred = model.predict(X_test) #returns a single discrete category that it has predicted\n",
        "    y_pred_proba = model.predict_proba(X_test)[:,1] #returns continuous values that represent the likelihood of each input belonging to each class\n",
        "\n",
        "    print(f'Accuracy score    : {accuracy_score(y_test, y_pred):.3}')\n",
        "    print(f'Recall score      : {recall_score(y_test, y_pred):.3}')\n",
        "    print(f'Precision score   : {precision_score(y_test, y_pred):.3}')\n",
        "    print(f'F1 score          : {f1_score(y_test, y_pred):.3}')\n",
        "    print(f'F2 score          : {fbeta_score(y_test, y_pred, beta=2):.3}')\n",
        "    print(f'Score metier      : {custom_metric(y_test, y_pred)}')\n",
        "    print(f'ROCAUC score      : {roc_auc_score(y_test, y_pred_proba):.3}')\n",
        "    print()\n",
        "\n",
        "    plt.figure(figsize=(6,4))\n",
        "\n",
        "    visualizer = ROCAUC(model, binary=True, random_state=101)\n",
        "\n",
        "    visualizer.fit(X_train, y_train)        # Fit the visualizer and the model\n",
        "    visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
        "\n",
        "    visualizer.show()"
      ],
      "metadata": {
        "id": "RKTm1m-_iVHn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}